= Guided solution (page 1)

. Log in to your lab environment on the **ROLE** platform.
. Break the environment if you have not done it and then step through the fix.
+
As the **student** user, run the lab start script on the **workstation** VM to reproduce the issue.
+
[source, bash]
----
cd /home/student/osp_training/scenarios_repo/
./lab start bfx019
----
+
.Sample output
----
[student@workstation scenarios_repo]$ ./lab start bfx019
Running start action against scenario bfx019
Run the following command:
ssh -i /home/student/osp_training/.scenariobfx019/scenario-bfx019-key.pem cirros@192.168.51.170
----
+
NOTE: The IP address in the displayed output may differ in your case.

. Run the ssh command from the previous lab command output and notice the Connection timed out error. Also run the ping command against instance IP.
+
[source, bash]
----
ssh -i /home/student/osp_training/.scenariobfx019/scenario-bfx019-key.pem cirros@instance.ip.in.your.lab
----
+
[source, bash]
----
ping instance.ip.in.your.lab -c 1
----
+
IMPORTANT: In the above commands **replace** the string *instance.ip.in.your.lab* with the **actual IP** address displayed in the output of the lab start script.
+
.Sample output
----
[student@workstation ~]$ ssh -i /home/student/osp_training/.scenariobfx019/scenario-bfx019-key.pem cirros@192.168.51.170
ssh: connect to host 192.168.51.170 port 22: Connection timed out

[student@workstation ~]$ ping 192.168.51.170 -c 1
PING 192.168.51.170 (192.168.51.170) 56(84) bytes of data.

--- 192.168.51.170 ping statistics ---
1 packets transmitted, 0 received, 100% packet loss, time 0ms
----
+
**You observe that both ping and ssh attempts failed which indicates lack of connectivity.**

. Access the RHOSO environment and determine the specific compute node where the instance is currently running.
+
[source, bash]
----
oc exec -n openstack openstackclient -- openstack server list --long -c Name -c Status -c Host
----
+
.Sample output
----
+--------------------+--------+---------------------------+
| Name               | Status | Host                      |
+--------------------+--------+---------------------------+
| scenario-bfx019-vm | ACTIVE | compute02.srv.example.com |
+--------------------+--------+---------------------------+
----
+
**Observe that it is compute02 in the above output, it my differ in your case.**

. Make sure you have access to compute node from other terminal before proceeding further.
+
[source, bash]
----
ssh root@compute02.srv.example.com
----
+
**Make sure you ssh to the appropriate compute node for your lab environment.**
+
.Sample output
----
[student@workstation ~]$ ssh root@compute02.srv.example.com
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Fri Apr  4 08:32:38 2025 from 192.168.51.254
[root@compute02 ~]#
----

. Exit from the compute node to be back on the workstation VM.

. Connect to the utility vm over ssh and check the route taken to reach the instance from the utility VM.
+
[source, bash]
----
ssh lab@utility
----
+
[source, bash]
----
ip route get instance.ip.in.your.lab
----
+
**Replace instance.ip.in.your.lab with the actual IP address.**
+
NOTE: Only the `utility` server has configured IP address which belongs to the same Layer 2 (L2) domain as the Floating IP associated to the VM.
+
----
[student@workstation ~]$ ssh lab@utility
Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Wed Apr  2 06:32:08 2025 from 172.25.250.9

[lab@utility ~]$ ip route get 192.168.51.170
192.168.51.170 dev eth2 src 192.168.51.254 uid 1002
----
+
**The above output shows that the route goes through eth2.**

. Confirm that the utility server and the instance's floating IP are on the same Layer 2 (L2) domain, specifically on the 192.168.51.0 subnet.
+
[source, bash]
----
ip addr show eth2
----
+
.Sample output
----
[lab@utility ~]$ ip addr show eth2
4: eth2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:02:33:fe brd ff:ff:ff:ff:ff:ff
    altname enp0s5
    altname ens5
    inet 192.168.51.254/24 brd 192.168.51.255 scope global noprefixroute eth2
       valid_lft forever preferred_lft forever
    inet6 fe80::5ad2:878f:6d6a:530d/64 scope link noprefixroute
       valid_lft forever preferred_lft forever
----

. Examine the ARP cache on the utility server.
+
[source, bash]
----
ip neighbor | grep instance.ip.in.your.lab
----
+
**Replace instance.ip.in.your.lab with actual IP address**
+
.Sample output
----
[lab@utility ~]$ ip neighbor | grep 192.168.51.170
192.168.51.170 dev eth2 lladdr fa:16:3e:42:c2:37 STALE
----
+
* An entry with the floating IP is associated with a MAC address, indicating successful ARP negotiation.
* This signifies functional communication via ARP, but ICMP communication remains problematic.

. Exit from the utility VM and let us proceed to check whether the packets are being correctly routed to the appropriate node. 
. Run the below command on the compute node to check the interface associated with the bridge br-ex.
+
[source, bash]
----
ssh root@compute02.srv.example.com
----
+
[source, bash]
----
ovs-vsctl show
----
+
.Sample output
----
[student@workstation ~]$ ssh root@compute02.srv.example.com
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Fri Apr  4 08:41:58 2025 from 192.168.51.254


[root@compute02 ~]# ovs-vsctl show
54ee4d39-0f5f-4be7-9253-e7963bf0ec50
    Manager "ptcp:6640:127.0.0.1"
        is_connected: true
    Bridge br-ex
        fail_mode: standalone
        Port patch-provnet-84157851-395c-40eb-a3ec-6b512dd58759-to-br-int
            Interface patch-provnet-84157851-395c-40eb-a3ec-6b512dd58759-to-br-int
                type: patch
                options: {peer=patch-br-int-to-provnet-84157851-395c-40eb-a3ec-6b512dd58759}
        Port br-ex
            Interface br-ex
                type: internal
        Port eth2
            Interface eth2
----
+
**As per the above output, note that interface eth2 is associated with the bridge br-ex.**

. To verify incoming traffic, initiate a tcpdump on this interface on the compute node and try to ping the instance from another terminal from the workstation node.
+
**On compute node:**
+
[source, bash]
----
tcpdump -envvi eth2 icmp
----
+
**On workstation VM:**
+
[source, bash]
----
ping instance.ip.in.your.lab
----
+
.Sample output
----
[root@compute02 ~]# tcpdump -envvi eth2 icmp
dropped privs to tcpdump
tcpdump: listening on eth2, link-type EN10MB (Ethernet), snapshot length 262144 bytes
08:51:14.195753 52:54:00:02:33:fe > fa:16:3e:42:c2:37, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 63, id 5314, offset 0, flags [DF], proto ICMP (1), length 84)
    172.25.250.9 > 192.168.51.170: ICMP echo request, id 3, seq 1, length 64
08:51:15.231845 52:54:00:02:33:fe > fa:16:3e:42:c2:37, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 63, id 5801, offset 0, flags [DF], proto ICMP (1), length 84)
    172.25.250.9 > 192.168.51.170: ICMP echo request, id 3, seq 2, length 64
08:51:16.255850 52:54:00:02:33:fe > fa:16:3e:42:c2:37, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 63, id 6204, offset 0, flags [DF], proto ICMP (1), length 84)
    172.25.250.9 > 192.168.51.170: ICMP echo request, id 3, seq 3, length 64
08:51:17.279535 52:54:00:02:33:fe > fa:16:3e:42:c2:37, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 63, id 6347, offset 0, flags [DF], proto ICMP (1), length 84)
    172.25.250.9 > 192.168.51.170: ICMP echo request, id 3, seq 4, length 64
----
+
* You observe that ICMP echo requests arriving at the machine.
* The presence of ICMP echo requests reaching the external NIC on the compute node indicates the proper functioning of the Distributed Virtual Router (DVR).
* However, you observed that echo requests are not receiving the echo replies on the workstation VM.

. Determine tap interface used for the instance on the compute node.
. Run below command on the workstation VM.
+
[source, bash]
----
oc exec -n openstack openstackclient -- openstack port list --server scenario-bfx019-vm
----
+
.Sample output
----
[student@workstation ~]$ oc exec -n openstack openstackclient -- openstack port list --server scenario-bfx019-vm
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+
| ID                                   | Name | MAC Address       | Fixed IP Addresses                                                            | Status |
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+
| 382179f5-4596-44b2-b0cf-27c8cb9fb2f3 |      | fa:16:3e:ef:f8:14 | ip_address='192.168.110.95', subnet_id='f57aa9b8-dbb8-4d29-ad79-746ce91cbd7b' | ACTIVE |
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+
----

. Derive the tap device's name by appending "tap" to the initial string from the port ID and check it's network interface setting **on the compute node**.
+
.Sample output
----
[root@compute02 ~]# ip link show tap382179f5-45
14: tap382179f5-45: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1442 qdisc noqueue master ovs-system state UNKNOWN mode DEFAULT group default qlen 1000
    link/ether fe:16:3e:ef:f8:14 brd ff:ff:ff:ff:ff:ff
[root@compute02 ~]#
----
+
Note how tap deviceâ€™s name is derived by appending **tap** to the initial string **382179f5-45** from the port ID. 
You need to replace this string as per the port ID in your lab.

. Run a **tcpdump** on the tap interface of the **compute** node while initiating a **ping** to the instance from the **workstation** VM.
+
.Sample output
----
[root@compute02 ~]# tcpdump -envvi tap382179f5-45
dropped privs to tcpdump
tcpdump: listening on tap382179f5-45, link-type EN10MB (Ethernet), snapshot length 262144 bytes
----
+
----
[student@workstation ~]$ ping 192.168.51.170 -c 1
PING 192.168.51.170 (192.168.51.170) 56(84) bytes of data.

--- 192.168.51.170 ping statistics ---
1 packets transmitted, 0 received, 100% packet loss, time 0ms

[student@workstation ~]$ 
----
+
You see that on ICMP echo requests are visible on the external NIC, they are not being delivered to the instance (via tap interface). The issue seems to be occurring within the br-int bridge where packets appear to be lost.

. **On the compute node**, run the following command to start `tcpdump` and capture a single ICMP request packet
+
[source, bash]
----
sudo tcpdump -i <external_interface> icmp -c 1 -w icmp-request.pcap
----
+
**Replace <external_interface>** with the name of the appropriate external network interface (e.g., `eth2`, `ens4`, etc.) associated with br-ex.
+
.Sample output
----
[root@compute02 ~]# tcpdump -i eth2 icmp -c1 -w icmp-request.pcap
dropped privs to tcpdump
tcpdump: listening on eth2, link-type EN10MB (Ethernet), snapshot length 262144 bytes
1 packet captured
1 packet received by filter
0 packets dropped by kernel
----

. **From the workstation node**, send a single ICMP request (ping) to the instance
+
[source, bash]
----
ping -c 1 <instance_ip>
----
+
**Replace <instance_ip>** with the IP address of the target instance.

. Once the ping is sent, the `tcpdump` command on the compute node will capture the ICMP request and save it to the file `icmp-request.pcap`.
+
.Sample output
----
[root@compute02 ~]# tcpdump -i eth2 icmp -c1 -w icmp-request.pcap
dropped privs to tcpdump
tcpdump: listening on eth2, link-type EN10MB (Ethernet), snapshot length 262144 bytes
1 packet captured
1 packet received by filter
0 packets dropped by kernel
----

. Verify the generated file content by reading back the stored packet using tcpdump command **on the compute node**.
+
[source, bash]
----
tcpdump -r icmp-request.pcap
----
+
.Sample output
----
[root@compute02 ~]# tcpdump -r icmp-request.pcap
reading from file icmp-request.pcap, link-type EN10MB (Ethernet), snapshot length 262144
dropped privs to tcpdump
08:59:57.952623 IP workstation.lab.example.com > 192.168.51.194: ICMP echo request, id 5, seq 1, length 64
----

. Use the ovs-pcap tool to read and display the packets from the stored icmp-request.pcap file.
+
[source, bash]
----
ovs-pcap icmp-request.pcap
----
+
.Sample output
----
[root@compute02 ~]# ovs-pcap icmp-request.pcap
fa163e42c2375254000233fe0800450000546de440003f013337ac19fa09c0a833c208007f98000500018d9fef67000000002e870e0000000000101112131415161718191a1b1c1d1e1f202122232425262728292a2b2c2d2e2f3031323334353637
----
+
* **ovs-pcap** command is provided by **openvswitch-test** package.
* openvswitch-test package is available form the *fast-datapath-for-rhel-9-x86_64-rpms* channel.
* Subscribe the compute node and enable fast-datapath-for-rhel-9-x86_64-rpms channel.
* Install **openvswitch3.3-test-3.3.0-49.el9fdp.noarch** package.
+
[source, bash]
----
subscription-manager register
subscription-manager repos --enable=fast-datapath-for-rhel-9-x86_64-rpms
dnf install openvswitch3.3-test-3.3.0-49.el9fdp.noarch
----
