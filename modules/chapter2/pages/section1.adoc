= Guided solution (page 1)

== Objectives
* Investigate the instance connectivity issue in Red Hat OpenStack Platform.
* Solve the instance connectivity in the hands-on lab environment.

== Outcomes
* Investigate why ssh to instance is not working.
* Fix the ssh to instance failing issue.

== Instructions

. Break the environment if you have not done it and then step through the fix.
+
[source, bash]
----
lab start bfx019
----
+
.Sample output
----
[student@workstation ~]$ lab start bfx019
Running start action against scenario bfx019
Run the following command:
ssh -i /home/student/osp_training/.scenariobfx019/scenario-bfx019-key.pem cirros@192.168.51.194
----
+
NOTE: The IP address in the displayed output may differ in your case.

. Run the ssh command from the previous lab command output and notice the Connection timed out error. Also run the ping command against instance IP.
+
[source, bash]
----
ssh -i /home/student/osp_training/.scenariobfx019/scenario-bfx019-key.pem cirros@instance.ip.in.your.lab
ping instance.ip.in.your.lab -c 1
----
+
IMPORTANT: In the above commands replace the string _instance.ip.in.your.lab_ with the actual IP address displayed in the output of the lab start script.
+
.Sample output
----
[student@workstation ~]$ ssh -i /home/student/osp_training/.scenariobfx019/scenario-bfx019-key.pem cirros@192.168.51.194
*
ssh: connect to host 192.168.51.194 port 22: Connection timed out

[student@workstation ~]$ ping 192.168.51.194 -c 1
PING 192.168.51.194 (192.168.51.194) 56(84) bytes of data.

--- 192.168.51.194 ping statistics ---
1 packets transmitted, 0 received, 100% packet loss, time 0ms
----
+
**You observe that both ping and ssh attempts failed which indicates lack of connectivity.**

. Access the RHOSO environment and determine the specific compute node where the instance is currently running.
+
[source, bash]
----
oc exec -n openstack openstackclient -- openstack server list --long -c Name -c Status -c Host
----
+
.Sample output
----
+--------------------+--------+---------------------------+
| Name               | Status | Host                      |
+--------------------+--------+---------------------------+
| scenario-bfx019-vm | ACTIVE | compute01.srv.example.com |
+--------------------+--------+---------------------------+
----
+
**Observe that it is compute01 in the above output, it my differ in your case.**

. Make sure you have access to compute node from other terminal before proceeding further.
+
[source, bash]
----
ssh root@compute01.srv.example.com
----
+
**Make sure you ssh to the appropriate compute node for your lab environment.**
+
.Sample output
----
[student@workstation ~]$ ssh root@compute01.srv.example.com
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Fri Apr  4 08:32:38 2025 from 192.168.51.254
[root@compute01 ~]#
----

. Exit from the compute node to be back on the workstation VM.

. Connect to the utility vm over ssh and check the route taken to reach the instance from the utility VM.
+
[source, bash]
----
ssh lab@utility
----
+
[source, bash]
----
ip route get instance.ip.in.your.lab
----
+
__Replace _instance.ip.in.your.lab_ with the actual IP address.__
+
NOTE: Only the `utility` server has configured IP address which belongs to the same Layer 2 (L2) domain as the Floating IP associated to the VM.
+
----
[student@workstation ~]$ ssh lab@utility
Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Wed Apr  2 06:32:08 2025 from 172.25.250.9

[lab@utility ~]$ ip route get 192.168.51.194
192.168.51.194 dev eth2 src 192.168.51.254 uid 1002
----
+
**The above output shows that the route goes through eth2.**

. Confirm that the utility server and the instance's floating IP are on the same Layer 2 (L2) domain, specifically on the 192.168.51.0 subnet.
+
[source, bash]
----
ip addr show eth2
----
+
.Sample output
----
[lab@utility ~]$ ip addr show eth2
4: eth2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:02:33:fe brd ff:ff:ff:ff:ff:ff
    altname enp0s5
    altname ens5
    inet 192.168.51.254/24 brd 192.168.51.255 scope global noprefixroute eth2
       valid_lft forever preferred_lft forever
    inet6 fe80::5ad2:878f:6d6a:530d/64 scope link noprefixroute
       valid_lft forever preferred_lft forever
----

. Examine the ARP cache on the utility server.
+
[source, bash]
----
ip neighbor | grep instance.ip.in.your.lab
----
+
_Replace instance.ip.in.your.lab with actual IP address_
+
.Sample output
----
[lab@utility ~]$ ip neighbor | grep 192.168.51.194
192.168.51.194 dev eth2 lladdr fa:16:3e:42:c2:37 STALE
----
+
* An entry with the floating IP is associated with a MAC address, indicating successful ARP negotiation.
* This signifies functional communication via ARP, but ICMP communication remains problematic.

. Exit from the utility VM and let us proceed to check whether the packets are being correctly routed to the appropriate node. 
. Run the below command on the compute node to check the interface associated with the bridge br-ex.
+
[source, bash]
----
ssh root@compute01.srv.example.com
----
+
[source, bash]
----
ovs-vsctl show
----
+
.Sample output
----
[student@workstation ~]$ ssh root@compute01.srv.example.com
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Fri Apr  4 08:41:58 2025 from 192.168.51.254


[root@compute01 ~]# ovs-vsctl show
54ee4d39-0f5f-4be7-9253-e7963bf0ec50
    Manager "ptcp:6640:127.0.0.1"
        is_connected: true
    Bridge br-ex
        fail_mode: standalone
        Port patch-provnet-84157851-395c-40eb-a3ec-6b512dd58759-to-br-int
            Interface patch-provnet-84157851-395c-40eb-a3ec-6b512dd58759-to-br-int
                type: patch
                options: {peer=patch-br-int-to-provnet-84157851-395c-40eb-a3ec-6b512dd58759}
        Port br-ex
            Interface br-ex
                type: internal
        Port eth2
            Interface eth2
----
+
**As per the above output, note that interface eth2 is associated with the bridge br-ex.**

. To verify incoming traffic, initiate a tcpdump on this interface on the compute node and try to ping the instance from another terminal from the workstation node.
+
On compute node:
+
[source, bash]
----
tcpdump -envvi eth2 icmp
----
+
On workstation VM:
+
[source, bash]
----
ping instance.ip.in.your.lab
----
+
.Sample output
----
[root@compute01 ~]# tcpdump -envvi eth2 icmp
dropped privs to tcpdump
tcpdump: listening on eth2, link-type EN10MB (Ethernet), snapshot length 262144 bytes
08:51:14.195753 52:54:00:02:33:fe > fa:16:3e:42:c2:37, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 63, id 5314, offset 0, flags [DF], proto ICMP (1), length 84)
    172.25.250.9 > 192.168.51.194: ICMP echo request, id 3, seq 1, length 64
08:51:15.231845 52:54:00:02:33:fe > fa:16:3e:42:c2:37, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 63, id 5801, offset 0, flags [DF], proto ICMP (1), length 84)
    172.25.250.9 > 192.168.51.194: ICMP echo request, id 3, seq 2, length 64
08:51:16.255850 52:54:00:02:33:fe > fa:16:3e:42:c2:37, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 63, id 6204, offset 0, flags [DF], proto ICMP (1), length 84)
    172.25.250.9 > 192.168.51.194: ICMP echo request, id 3, seq 3, length 64
08:51:17.279535 52:54:00:02:33:fe > fa:16:3e:42:c2:37, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 63, id 6347, offset 0, flags [DF], proto ICMP (1), length 84)
    172.25.250.9 > 192.168.51.194: ICMP echo request, id 3, seq 4, length 64
----
+
* You observe that ICMP echo requests arriving at the machine.
* The presence of ICMP echo requests reaching the external NIC on the compute node indicates the proper functioning of the Distributed Virtual Router (DVR).
* However, you observed that echo requests are not receiving the echo replies on the workstation VM.
